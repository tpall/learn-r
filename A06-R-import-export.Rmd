---

---

# Getting data in and out of R

![You know nothing. Official promotion logo for Banksy's October 2013 artist residency in New York City. Photo: Banksy](https://upload.wikimedia.org/wikipedia/en/2/2c/Banksy_Better_Out_Than_In_New_York_City_2013.png)

<!-- When planning to import your data from external files to R, then -->

<!-- + avoid whitespaces in your file names, replace these with `.`,`_` or `-` -->
<!-- + don't use special characters like `,` `?` `#` `*` `@` `$` `%` `|` `^` `{` `}` `(` `)` `[` `]` `\` `/` -->

## Import tabular data
`read.table`, `read.csv`, `read.delim` functions allow to create data frames, where different columns may contain different type of data -- `numeric`, `character` etc. 
`read.table` is the basic function with values separated by white space `""` (one or more spaces, tabs, newlines). 
`read.csv` is a wrapper around it and expects comma `,` as a field separator and `read.delim` expects `tab` separator `\t`.

Other important arguments of `read.table` are:
+ `dec = "."` the character used in the file for decimal points. In many cases ignorant people use comma as decimal separator.
+ `stringsAsFactors = ` default setting is TRUE and character data is converted into factors.
+ `na.string = "NA"` a character vector of strings which are to be interpreted as NA values. Blank fields are also considered to be missing values in logical, integer, numeric and complex fields.  
+ `skip =` the number of lines of the data file to skip before beginning to read data.

We use survey data (%) of eating fruits and vegetables within last 7 days from Estonian [Institute for Health Development](http://www.tai.ee/en/). 
Don't mind the file extension .csv, it's values are tab separated.
TAI offers different download formats, but mostly in useless forms (even for .csv and .txt files).
Only "Tabeldieraldusega pealkirjata tekst (.csv)" and "Semikooloneraldusega pealkirjata tekst (.csv)" are in a suitable rectangular format, although lacking column headers.
We have to identify and add column headers separately and fix character encoding.

```{r}
fruit <- read.table("data/TKU10m.csv") # tab separated text
colnames(fruit) <- c("Year", "Foodstuff", "Consumption", "Gender", "AGE16-24", "AGE25-34", "AGE35-44", "AGE45-54", "AGE55-64")
head(fruit)
# Lets translate some variables to english by changing factor labels
fruit$Foodstuff  <- factor(fruit$Foodstuff, levels = c("K\xf6\xf6givili","Puuvili"), labels = c("Vegetables", "Fruits"))
fruit$Consumption <-  factor(fruit$Consumption, levels = c("Ei s\xf6\xf6nud", "1-2 p\xe4eval", "3-5 p\xe4eval", "6-7 p\xe4eval"), labels = c("No", "1-2 days", "3-5 days", "6-7 days"))
fruit$Gender <-  factor(fruit$Gender, levels = c("Mehed", "Naised"), labels = c("Males", "Females"))                            
head(fruit)
```

Table of downloadable R .csv datasets to play around and test things is for example available [here](https://vincentarelbundock.github.io/Rdatasets/datasets.html).
As you can see, you can use URL to download data directly from web.

```{r}
airquality <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/datasets/airquality.csv")
head(airquality)
```

### `readr` package
You can import tabular data using read_ functions from `readr` package. Compared to base R functions like `read.csv()`, `readr` is much faster (important for very large datasets) and gives more convenient output: it never converts strings to factors, can parse date/times, and it leaves the column names as in raw data. 

We can compare what happens with column names in case of `read.csv` and `read_csv`:

`base::read.csv` changes column names (1st row):
```{r}
read.csv(textConnection("1 column, my data
               2,3
               4,5"))
```

`readr::read_csv` leaves column names intact:
```{r}
library(readr)
read_csv("1 column, my data
              2,3
              4,5")  
```

Note also that in case of `read_csv` you can directly paste your comma separated text into function (instead trough textConnection). 

The first two arguments of `read_csv()` are:
+ `file`: path (or URL) to the file you want to load. Readr can automatically decompress files ending in .zip, .gz, .bz2, and .xz.
+ `col_names`: column names. 3 options: TRUE (the default); FALSE numbers columns sequentially from X1 to Xn. A character vector, used as column names. If these don't match up with the columns in the data, you'll get a warning message.

<!-- `read_fwf()` reads fixed width files. You can specify fields either by their widths with `fwf_widths()` or their position with `fwf_positions()`.  -->
`read_table()` reads a common variation of fixed width files where columns are separated by white space.


```{r, eval = FALSE}
install.packages("readr")
library(readr)
read_table() # read the type of textual data where each column is separate by whitespace
read_csv() # reads comma delimited files, 
read_tsv() # reads tab delimited files, 
read_delim() # reads in files with a user supplied delimiter.
```

Importantly, `read_` functions expect specific delimiter: comma for _csv, tab for _tsv etc., and only `read_delim` has argument for specifying delimiter to be used.

```{r, include = FALSE}
library(readr)
```

<!-- #### Change column type -->
<!-- ```{r, eval=FALSE} -->
<!-- read_csv("mypath.csv", col_types = col(x = col_integer(), treatment = col_character())) -->
<!-- ``` -->

<!-- You can specify the following types of columns: -->

<!-- + `col_integer()` (i) and `col_double()` (d) specify integer and doubles. -->
<!-- + `col_logical()` (l) parses TRUE, T, FALSE and F into a logical vector. -->
<!-- + `col_character()` (c) leaves strings as is. -->
<!-- + `col_number()` (n) is a more flexible parser for numbers embedded in other strings. It will look for the first number in a string, ignoring non-numeric prefixes and suffixes. -->
<!-- + `col_factor()` (f) allows you to load data directly into a factor if you know what the levels are. -->
<!-- + `col_skip()` (_, -) completely ignores a column. -->
<!-- + `col_date()` (D), `col_datetime()` (T) and `col_time()` (t) parse into dates, date times, and times. -->

<!-- Each column parser has a one letter abbreviation, which you can use instead of the full function call (assuming you're happy with the default arguments): -->

<!-- ```{r, eval=FALSE} -->
<!-- read_csv("mypath.csv", col_types = cols(x = "i", treatment = "c")) -->
<!-- ``` -->

<!-- Each `col_XYZ()` function also has a corresponding `parse_XYZ()` that you can use on a character vector. -->

<!-- ```{r} -->
<!-- parse_integer(c("1", "2", "3")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- parse_logical(c("TRUE", "FALSE", "NA")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- parse_number(c("$1000", "20%", "3,000")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- parse_datetime("2010-10-01T2010") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- parse_date("2010-10-01") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- parse_time("20:10:01") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- parse_date("01/02/15", "%m/%d/%y") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- parse_date("01/02/15", "%d/%m/%y") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- parse_date("01/02/15", "%y/%m/%d") -->
<!-- ``` -->


## Import MS Excel
There are several libraries and functions available to import MS excel workbooks into R, like `XLConnect`,`gdata::read.xls()`, `xlsx`. 
`XLConnect` is a powerful package for working with .xls(x) files, but it depends on Java and has memory limitations: you'll never know when your script crashes. 
`readxl` package contains only two verbs and is very easy to use. 

```{r}
library(readxl)
xlsfile <- "data/ECIS_140317_MFT_1.xls" # 96-well multi frequency real-time impedance data
sheets <- excel_sheets(xlsfile) 
sheets
z <- read_excel(xlsfile, sheets[3]) # we import 3rd sheet "Z 1000 Hz"
dim(z)
```


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Extract tables from messy spreadsheets with jailbreakr <a href="https://t.co/9wJfDj0cLM">https://t.co/9wJfDj0cLM</a> <a href="https://twitter.com/hashtag/rstats?src=hash">#rstats</a> <a href="https://twitter.com/hashtag/DataScience?src=hash">#DataScience</a></p>&mdash; R-bloggers (@Rbloggers) <a href="https://twitter.com/Rbloggers/status/766226281859997697">August 18, 2016</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js"charset="utf-8"></script>

## Import ODS
To import Open Document Spreadsheets .ods files into R you can try [following approach](https://github.com/chainsawriot/readODS).
```{r, eval = FALSE}
library(readODS)
read_ods("table.ods", header = TRUE) ## return only the first sheet
read_ods("multisheet.ods", sheet = 2) ## return the second sheet 
```

<!-- ## PDF tables -->
<!-- PDF table extractor -- `Tabula` - exists as both a server application, accessed via a web browser, or as a service using the tabula extractor Java application. The (`tabulizer` R package)[https://github.com/ropenscilabs/tabulizer] provides a wrapper for tabula extractor (bundled within the package), that lets you access the service via it's command line calls. (One dependency you do need to take care of is to have Java installed; adding Java into an RStudio docker container would be one way of taking care of this.) -->

<!-- You can try this code with [more instructions on `github`](https://github.com/ropenscilabs/tabulizer) to install package (if rJava is installed successfully...). -->
<!-- ```{r, eval=FALSE} -->
<!-- devtools::install_github("ropenscilabs/tabulizer") -->
<!-- library("tabulizer") -->
<!-- f <- system.file("examples", "data.pdf", package = "tabulizer") -->
<!-- out1 <- extract_tables(f) -->
<!-- str(out1) -->
<!-- ``` -->

## Import SPSS, SAS etc.
`foreign` package provies functions for reading and writing data stored by Minitab, S, SAS, SPSS, Stata, etc.

```{r, eval=FALSE}
library(foreign)
mydata <- read.spss("mydata.sav") # import spss data file, returns list
mydata <- read.spss("mydata.sav", to.data.frame = TRUE) # returns data.frame
```

## Import all datasets from directory
We can use `sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)` or `lapply(X, FUN, ...)` functions to iterate through vector or list of files, respectively.
Three dots `...` shows that you can pass further arguments to your function (FUN).

```{r, include=FALSE, eval=FALSE}
# First we define `read_datasets` function, which takes in as arguments vector of filenames and reader function.
library("readr")
library("tibble")
read_datasets <- function(datasets, read_func){
    read_and_assign <- function(dataset, read_func){
        assign(dataset, read_func(dataset), envir = .GlobalEnv)
    }
    output <- invisible(
    sapply(datasets, read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))
  }
```


```{r}
data_files <- list.files(path = "data", pattern = ".csv", full.names = TRUE) # 
data_files # ups, we have only one file
datasets <- sapply(data_files, read.table, simplify = FALSE, USE.NAMES = TRUE) # sapply returns vector or matrix, simplify = FALSE outputs list
str(datasets)
```

<!-- If you have datasets with same column names, you can import all datasets as a list and rbind them into one data.frame. Here we concatenate our single data.frame in `datasets` object. -->

<!-- `do.call(rbind, my_list_of_dataframes)` is base R solution to this task. -->
<!-- ```{r} -->
<!-- nrow(datasets[[1]]) # how many rows in our original data? datasets is a list. -->
<!-- rbound_datasets <-  do.call(rbind, c(datasets, datasets)) # bind dataframes in list by rows -->
<!-- nrow(rbound_datasets) # how many rows we have now -->
<!-- ``` -->

<!-- Alternative solution is `bind_rows` from `dplyr` package. -->
<!-- ```{r} -->
<!-- rbound_datasets2 <- dplyr::bind_rows(c(datasets, datasets)) -->
<!-- nrow(rbound_datasets2) # how many rows we have now -->
<!-- ``` -->

## Import text file
Probably, the most basic form of data to import into R is a simple text file.

Here we write our data to external file `ex.data` and read it into R using `scan()` function. 
Importantly, `scan()` reads vectors of data which all have the same mode. 
Default data type is numeric, strings can be specified with the `what = ""` argument.
```{r}
cat("my title line", "2 3 5 7", "11 13 17", file = "ex.data", sep = "\n")
pp <- scan("ex.data", skip = 1) # we skip 1st line with title text or we get error
unlink("ex.data") # tidy up, unlink deletes the file(s) or directories specified
pp
```

In case you dont wan't or can't save your text into file (bad for reproducibility!), it's possible to use `textConnection()` function to input data into R. 
`\n` is a newline character. 

`readLines` reads "unorganized" data, this is the function that will read input into R so that we can manipulate it further.

```{r}
zzz <- textConnection("my title line 2 3 5 7 11 13 17 9") 
pp <- readLines(zzz) # zzz is a connection object
pp
close(zzz) # close connection
```

```{r}
pp <- scan(textConnection("my title line\n2 3 5 7\n11 13 17 9"), skip = 1)
pp
```

Text in `textConnection` call can be already structured, so you can quickly import copy-paste data from screen into R. 
```{r}
zzz <- textConnection("my title line 
                      2 3 5 7
                      11 13 17 9")
a <- scan(zzz, skip = 2) # lets skip 1st two lines
a
```

Scanned data can be coerced into rectangular matrix. 
We have 2 rows of numbers in our text string shown above therefore we set `nrow = 2` and we need to specify that data is inserted into matrix rowwise `byrow = TRUE` (default option is FALSE) to keep original data structure.

```{r}
matrix(pp, nrow = 2, byrow = TRUE)
```
